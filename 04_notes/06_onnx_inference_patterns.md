# ONNXæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³å®Œå…¨ã‚¬ã‚¤ãƒ‰

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€ONNXãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ãŸæ¨è«–ã®æ§˜ã€…ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã¨å®Ÿè£…æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

## ğŸ“‹ ç›®æ¬¡

1. [ONNXåŸºç¤](#onnxåŸºç¤)
2. [æ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸€è¦§](#æ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸€è¦§)
3. [ãƒ‘ã‚¿ãƒ¼ãƒ³1: åŒæœŸæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³](#ãƒ‘ã‚¿ãƒ¼ãƒ³1-åŒæœŸæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³)
4. [ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒãƒƒãƒæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³](#ãƒ‘ã‚¿ãƒ¼ãƒ³2-ãƒãƒƒãƒæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³)
5. [ãƒ‘ã‚¿ãƒ¼ãƒ³3: éåŒæœŸæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³](#ãƒ‘ã‚¿ãƒ¼ãƒ³3-éåŒæœŸæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³)
6. [ãƒ‘ã‚¿ãƒ¼ãƒ³4: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³](#ãƒ‘ã‚¿ãƒ¼ãƒ³4-ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³)
7. [ãƒ‘ã‚¿ãƒ¼ãƒ³5: REST APIãƒ‘ã‚¿ãƒ¼ãƒ³](#ãƒ‘ã‚¿ãƒ¼ãƒ³5-rest-apiãƒ‘ã‚¿ãƒ¼ãƒ³)
8. [ãƒ‘ã‚¿ãƒ¼ãƒ³6: gRPCãƒ‘ã‚¿ãƒ¼ãƒ³](#ãƒ‘ã‚¿ãƒ¼ãƒ³6-grpcãƒ‘ã‚¿ãƒ¼ãƒ³)
9. [ãƒ‘ã‚¿ãƒ¼ãƒ³7: ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³](#ãƒ‘ã‚¿ãƒ¼ãƒ³7-ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³)
10. [ãƒ‘ã‚¿ãƒ¼ãƒ³é¸æŠã‚¬ã‚¤ãƒ‰](#ãƒ‘ã‚¿ãƒ¼ãƒ³é¸æŠã‚¬ã‚¤ãƒ‰)
11. [å®Ÿè£…é †åºã®æ¨å¥¨](#å®Ÿè£…é †åºã®æ¨å¥¨)
12. [ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹](#ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹)
13. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)

---

## ONNXåŸºç¤

### ONNXã¨ã¯ï¼Ÿ

**ONNX (Open Neural Network Exchange)** ã¯ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ç›¸äº’é‹ç”¨å¯èƒ½ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã™ã€‚

#### ãªãœONNXã‚’ä½¿ã†ã®ã‹ï¼Ÿ

| è¦³ç‚¹ | ãƒ¡ãƒªãƒƒãƒˆ |
|------|---------|
| **ç§»æ¤æ€§** | è¨“ç·´ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆPyTorch, TensorFlow, scikit-learnï¼‰ã¨æ¨è«–ç’°å¢ƒã‚’åˆ†é›¢ |
| **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹** | ONNX Runtimeã¯é«˜åº¦ã«æœ€é©åŒ–ã•ã‚Œã¦ãŠã‚Šã€å…ƒã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚ˆã‚Šé«˜é€Ÿãªå ´åˆãŒå¤šã„ |
| **ãƒ‡ãƒ—ãƒ­ã‚¤æŸ”è»Ÿæ€§** | C++, Java, JavaScriptç­‰ã€æ§˜ã€…ãªè¨€èªã§æ¨è«–å¯èƒ½ |
| **ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–** | ã‚°ãƒ©ãƒ•æœ€é©åŒ–ã€é‡å­åŒ–ã€ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãªã©ã®æœ€é©åŒ–æŠ€è¡“ã‚’é©ç”¨å¯èƒ½ |

### ONNX Runtimeã¨ã¯ï¼Ÿ

**ONNX Runtime** ã¯ã€ONNXãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®é«˜æ€§èƒ½æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã§ã™ã€‚

#### ä¸»ãªç‰¹å¾´

- **ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ **: Windows, Linux, macOS, ãƒ¢ãƒã‚¤ãƒ«ã€Webãƒ–ãƒ©ã‚¦ã‚¶
- **ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**: CPU, GPU (CUDA), NPU, TPUã«å¯¾å¿œ
- **è¤‡æ•°è¨€èªå¯¾å¿œ**: Python, C++, C#, Java, JavaScript
- **æœ¬ç•ªç’°å¢ƒå¯¾å¿œ**: MicrosoftãŒé–‹ç™ºãƒ»ä¿å®ˆã—ã¦ãŠã‚Šã€å¤šãã®æœ¬ç•ªã‚·ã‚¹ãƒ†ãƒ ã§ä½¿ç”¨

### åŸºæœ¬çš„ãªä½¿ã„æ–¹

```python
import onnxruntime as ort
import numpy as np

# 1. ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
session = ort.InferenceSession('model.onnx')

# 2. å…¥åŠ›ãƒ»å‡ºåŠ›ã®æƒ…å ±å–å¾—
input_name = session.get_inputs()[0].name
output_name = session.get_outputs()[0].name

print(f"å…¥åŠ›å: {input_name}")
print(f"å…¥åŠ›å½¢çŠ¶: {session.get_inputs()[0].shape}")
print(f"å…¥åŠ›å‹: {session.get_inputs()[0].type}")

# 3. æ¨è«–å®Ÿè¡Œ
input_data = np.array([[5.1, 3.5, 1.4, 0.2]], dtype=np.float32)
outputs = session.run(None, {input_name: input_data})

# 4. çµæœå–å¾—
prediction = outputs[0]
print(f"äºˆæ¸¬çµæœ: {prediction}")
```

---

## æ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸€è¦§

| ãƒ‘ã‚¿ãƒ¼ãƒ³ | ç”¨é€” | è¤‡é›‘åº¦ | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ | å®Ÿè£…ã‚³ã‚¹ãƒˆ |
|---------|------|--------|--------------|-----------|
| **1. åŒæœŸæ¨è«–** | ãƒãƒƒãƒå‡¦ç†ã€å˜ç´”ãªAPI | â­ | â­â­ | ä½ |
| **2. ãƒãƒƒãƒæ¨è«–** | å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç† | â­â­ | â­â­â­â­â­ | ä½ |
| **3. éåŒæœŸæ¨è«–** | ä¸¦è¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç† | â­â­â­ | â­â­â­â­ | ä¸­ |
| **4. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ¨è«–** | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç† | â­â­â­â­ | â­â­â­â­ | é«˜ |
| **5. REST API** | Webã‚¢ãƒ—ãƒªã€ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ | â­â­ | â­â­â­ | ä½ |
| **6. gRPC** | é«˜æ€§èƒ½ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ | â­â­â­ | â­â­â­â­ | ä¸­ |
| **7. ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹** | ã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•ã€è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒ« | â­â­â­â­ | â­â­â­ | é«˜ |

---

## ãƒ‘ã‚¿ãƒ¼ãƒ³1: åŒæœŸæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³

### æ¦‚è¦

æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ã€‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆã”ã¨ã«1ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã€çµæœã‚’è¿”ã™ã¾ã§å¾…æ©Ÿã—ã¾ã™ã€‚

**iris_sklearn_svcãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§å®Ÿè£…æ¸ˆã¿**ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚

### é©ç”¨å ´é¢

- ãƒãƒƒãƒå‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- ä½ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ãªAPI
- é–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆç’°å¢ƒ
- ã‚·ãƒ³ãƒ—ãƒ«ãªCLIãƒ„ãƒ¼ãƒ«

### ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ

| ãƒ¡ãƒªãƒƒãƒˆ | ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ |
|---------|-----------|
| âœ… å®Ÿè£…ãŒç°¡å˜ | âŒ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒä½ã„ |
| âœ… ãƒ‡ãƒãƒƒã‚°ã—ã‚„ã™ã„ | âŒ ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡ãŒæ‚ªã„ |
| âœ… ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒç›´æ„Ÿçš„ | âŒ é«˜ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã«ä¸å‘ã |

### å®Ÿè£…ä¾‹

#### åŸºæœ¬çš„ãªå®Ÿè£…

```python
import onnxruntime as ort
import numpy as np
from typing import Dict, Any

class ONNXPredictor:
    """åŒæœŸæ¨è«–ã‚’è¡Œã†ã‚·ãƒ³ãƒ—ãƒ«ãªäºˆæ¸¬å™¨"""

    def __init__(self, model_path: str):
        """
        Args:
            model_path: ONNXãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name
        self.output_name = self.session.get_outputs()[0].name

    def predict(self, data: np.ndarray) -> np.ndarray:
        """
        æ¨è«–ã‚’å®Ÿè¡Œ

        Args:
            data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ (ä¾‹: shape=(1, 4) for iris)

        Returns:
            äºˆæ¸¬çµæœ
        """
        # å‹ã‚’ç¢ºèªãƒ»å¤‰æ›
        if data.dtype != np.float32:
            data = data.astype(np.float32)

        # æ¨è«–å®Ÿè¡Œ
        outputs = self.session.run(
            [self.output_name],
            {self.input_name: data}
        )

        return outputs[0]

    def predict_with_proba(self, data: np.ndarray) -> Dict[str, Any]:
        """
        äºˆæ¸¬ã¨ãã®ç¢ºç‡ã‚’è¿”ã™

        Args:
            data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿

        Returns:
            äºˆæ¸¬çµæœã¨ç¢ºç‡ã®è¾æ›¸
        """
        outputs = self.session.run(None, {self.input_name: data.astype(np.float32)})

        return {
            "prediction": outputs[0],
            "probabilities": outputs[1] if len(outputs) > 1 else None
        }

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # äºˆæ¸¬å™¨ã®åˆæœŸåŒ–
    predictor = ONNXPredictor("iris_model.onnx")

    # å˜ä¸€ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬
    sample = np.array([[5.1, 3.5, 1.4, 0.2]], dtype=np.float32)
    result = predictor.predict(sample)
    print(f"äºˆæ¸¬çµæœ: {result[0]}")

    # ç¢ºç‡ä»˜ãäºˆæ¸¬
    result_with_proba = predictor.predict_with_proba(sample)
    print(f"äºˆæ¸¬: {result_with_proba['prediction'][0]}")
    if result_with_proba['probabilities'] is not None:
        print(f"ç¢ºç‡: {result_with_proba['probabilities'][0]}")
```

#### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãå®Ÿè£…

```python
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RobustONNXPredictor:
    """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ­ã‚°ã‚’å‚™ãˆãŸåŒæœŸäºˆæ¸¬å™¨"""

    def __init__(self, model_path: str):
        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        if not Path(model_path).exists():
            raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")

        try:
            self.session = ort.InferenceSession(model_path)
            self.input_name = self.session.get_inputs()[0].name
            self.input_shape = self.session.get_inputs()[0].shape
            logger.info(f"ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æˆåŠŸ: {model_path}")
            logger.info(f"å…¥åŠ›å½¢çŠ¶: {self.input_shape}")
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {e}")
            raise

    def predict(self, data: np.ndarray) -> np.ndarray:
        try:
            # å…¥åŠ›æ¤œè¨¼
            self._validate_input(data)

            # å‹å¤‰æ›
            if data.dtype != np.float32:
                data = data.astype(np.float32)

            # æ¨è«–å®Ÿè¡Œ
            logger.debug(f"æ¨è«–é–‹å§‹: shape={data.shape}")
            outputs = self.session.run(None, {self.input_name: data})
            logger.debug(f"æ¨è«–å®Œäº†")

            return outputs[0]

        except ValueError as e:
            logger.error(f"å…¥åŠ›æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            raise
        except Exception as e:
            logger.error(f"æ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _validate_input(self, data: np.ndarray) -> None:
        """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼"""
        expected_features = self.input_shape[-1]

        if data.ndim != 2:
            raise ValueError(f"å…¥åŠ›ã¯2æ¬¡å…ƒé…åˆ—ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å®Ÿéš›: {data.ndim}æ¬¡å…ƒ")

        if data.shape[1] != expected_features:
            raise ValueError(
                f"ç‰¹å¾´é‡æ•°ãŒä¸æ­£ã§ã™ã€‚æœŸå¾…: {expected_features}, å®Ÿéš›: {data.shape[1]}"
            )

        # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
        if np.isnan(data).any():
            raise ValueError("å…¥åŠ›ã«æ¬ æå€¤(NaN)ãŒå«ã¾ã‚Œã¦ã„ã¾ã™")

        # ç„¡é™å¤§ãƒã‚§ãƒƒã‚¯
        if np.isinf(data).any():
            raise ValueError("å…¥åŠ›ã«ç„¡é™å¤§(inf)ãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
```

### iris_sklearn_svcã§ã®å®Ÿè£…

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€çµ±åˆãƒ†ã‚¹ãƒˆã§åŒæœŸæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ï¼š

```python
# tests/test_integration.py ã‹ã‚‰æŠœç²‹

def test_onnx_inference_matches_sklearn_prediction(
    self, trained_pipeline_and_test_data, onnx_model_path
):
    """ONNXã¨scikit-learnã®äºˆæ¸¬ãŒä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèª"""
    trained_model, x_test, _ = trained_pipeline_and_test_data

    # scikit-learnã§ã®äºˆæ¸¬
    sklearn_predictions = trained_model.predict(x_test)

    # ONNX Runtimeã§ã®äºˆæ¸¬ï¼ˆåŒæœŸæ¨è«–ï¼‰
    session = ort.InferenceSession(onnx_model_path)
    input_name = session.get_inputs()[0].name
    x_test_float32 = x_test.astype(np.float32)
    onnx_outputs = session.run(None, {input_name: x_test_float32})
    onnx_predictions = onnx_outputs[0]

    # äºˆæ¸¬çµæœã®ä¸€è‡´ã‚’ç¢ºèª
    assert np.array_equal(sklearn_predictions, onnx_predictions)
```

---

## ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒãƒƒãƒæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³

### æ¦‚è¦

è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã¾ã¨ã‚ã¦å‡¦ç†ã™ã‚‹ã“ã¨ã§ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã¾ã™ã€‚

### é©ç”¨å ´é¢

- å¤§é‡ã®ç”»åƒ/ãƒ†ã‚­ã‚¹ãƒˆã®ä¸€æ‹¬å‡¦ç†
- å®šæœŸçš„ãªãƒãƒƒãƒã‚¸ãƒ§ãƒ–
- ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ¨è«–
- ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ

| ãƒ¡ãƒªãƒƒãƒˆ | ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ |
|---------|-----------|
| âœ… ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒéå¸¸ã«é«˜ã„ | âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§ãŒä½ã„ |
| âœ… GPUæ´»ç”¨æ™‚ã«ç‰¹ã«åŠ¹æœçš„ | âŒ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤§ãã„ |
| âœ… ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡ãŒè‰¯ã„ | âŒ ãƒãƒƒãƒã‚µã‚¤ã‚ºã®èª¿æ•´ãŒå¿…è¦ |

### å®Ÿè£…ä¾‹

#### åŸºæœ¬çš„ãªãƒãƒƒãƒæ¨è«–

```python
import onnxruntime as ort
import numpy as np
from typing import List, Iterator
import logging

logger = logging.getLogger(__name__)

class BatchPredictor:
    """ãƒãƒƒãƒæ¨è«–ã‚’è¡Œã†äºˆæ¸¬å™¨"""

    def __init__(self, model_path: str, batch_size: int = 32):
        """
        Args:
            model_path: ONNXãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 32ï¼‰
        """
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name
        self.batch_size = batch_size
        logger.info(f"BatchPredictoråˆæœŸåŒ–: batch_size={batch_size}")

    def predict_batch(self, data_list: List[np.ndarray]) -> np.ndarray:
        """
        ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆã‚’ãƒãƒƒãƒå‡¦ç†

        Args:
            data_list: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ

        Returns:
            å…¨ã¦ã®äºˆæ¸¬çµæœã‚’çµåˆã—ãŸé…åˆ—
        """
        all_predictions = []

        # ãƒãƒƒãƒã«åˆ†å‰²ã—ã¦å‡¦ç†
        for batch in self._create_batches(data_list):
            batch_data = np.array(batch, dtype=np.float32)
            outputs = self.session.run(None, {self.input_name: batch_data})
            all_predictions.append(outputs[0])

        # çµæœã‚’çµåˆ
        return np.concatenate(all_predictions, axis=0)

    def _create_batches(self, data_list: List[np.ndarray]) -> Iterator[List[np.ndarray]]:
        """ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆã‚’ãƒãƒƒãƒã«åˆ†å‰²"""
        for i in range(0, len(data_list), self.batch_size):
            yield data_list[i:i + self.batch_size]

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    predictor = BatchPredictor("iris_model.onnx", batch_size=32)

    # 100å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆ
    samples = [np.array([5.1, 3.5, 1.4, 0.2]) for _ in range(100)]

    # ãƒãƒƒãƒæ¨è«–å®Ÿè¡Œ
    predictions = predictor.predict_batch(samples)
    print(f"å‡¦ç†å®Œäº†: {len(predictions)}ä»¶")
```

#### å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´ä»˜ãå®Ÿè£…

```python
import time
from typing import Optional

class AdaptiveBatchPredictor:
    """å‹•çš„ã«ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’èª¿æ•´ã™ã‚‹äºˆæ¸¬å™¨"""

    def __init__(
        self,
        model_path: str,
        initial_batch_size: int = 32,
        max_batch_size: int = 128,
        min_batch_size: int = 8
    ):
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name
        self.batch_size = initial_batch_size
        self.max_batch_size = max_batch_size
        self.min_batch_size = min_batch_size

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°
        self.avg_latency = None

    def predict_batch(self, data_list: List[np.ndarray]) -> np.ndarray:
        """å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºã§ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†"""
        all_predictions = []

        for batch in self._create_batches(data_list):
            start_time = time.time()

            # ãƒãƒƒãƒæ¨è«–
            batch_data = np.array(batch, dtype=np.float32)
            outputs = self.session.run(None, {self.input_name: batch_data})
            all_predictions.append(outputs[0])

            # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’è¨˜éŒ²
            latency = time.time() - start_time
            self._update_batch_size(latency, len(batch))

        return np.concatenate(all_predictions, axis=0)

    def _update_batch_size(self, latency: float, batch_size: int) -> None:
        """ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã«åŸºã¥ã„ã¦ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’èª¿æ•´"""
        # ç§»å‹•å¹³å‡ã‚’è¨ˆç®—
        if self.avg_latency is None:
            self.avg_latency = latency
        else:
            self.avg_latency = 0.9 * self.avg_latency + 0.1 * latency

        # ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´ãƒ­ã‚¸ãƒƒã‚¯
        per_sample_latency = latency / batch_size

        # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒä½ã„ = ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã™ä½™åœ°ã‚ã‚Š
        if per_sample_latency < 0.001 and self.batch_size < self.max_batch_size:
            self.batch_size = min(self.batch_size * 2, self.max_batch_size)
            logger.info(f"ãƒãƒƒãƒã‚µã‚¤ã‚ºå¢—åŠ : {self.batch_size}")

        # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒé«˜ã„ = ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™
        elif per_sample_latency > 0.01 and self.batch_size > self.min_batch_size:
            self.batch_size = max(self.batch_size // 2, self.min_batch_size)
            logger.info(f"ãƒãƒƒãƒã‚µã‚¤ã‚ºæ¸›å°‘: {self.batch_size}")

    def _create_batches(self, data_list: List[np.ndarray]) -> Iterator[List[np.ndarray]]:
        """ç¾åœ¨ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã§ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²"""
        for i in range(0, len(data_list), self.batch_size):
            yield data_list[i:i + self.batch_size]
```

#### ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ãå®Ÿè£…

```python
from tqdm import tqdm

class BatchPredictorWithProgress:
    """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ããƒãƒƒãƒäºˆæ¸¬å™¨"""

    def __init__(self, model_path: str, batch_size: int = 32):
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name
        self.batch_size = batch_size

    def predict_batch(
        self,
        data_list: List[np.ndarray],
        show_progress: bool = True
    ) -> np.ndarray:
        """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ãã§ãƒãƒƒãƒå‡¦ç†"""
        all_predictions = []
        batches = list(self._create_batches(data_list))

        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿
        iterator = tqdm(batches, desc="æ¨è«–ä¸­") if show_progress else batches

        for batch in iterator:
            batch_data = np.array(batch, dtype=np.float32)
            outputs = self.session.run(None, {self.input_name: batch_data})
            all_predictions.append(outputs[0])

        return np.concatenate(all_predictions, axis=0)

    def _create_batches(self, data_list: List[np.ndarray]) -> Iterator[List[np.ndarray]]:
        for i in range(0, len(data_list), self.batch_size):
            yield data_list[i:i + self.batch_size]

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    predictor = BatchPredictorWithProgress("iris_model.onnx", batch_size=32)

    # 1000å€‹ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å‡¦ç†
    samples = [np.array([5.1, 3.5, 1.4, 0.2]) for _ in range(1000)]
    predictions = predictor.predict_batch(samples)
    # å‡ºåŠ›: æ¨è«–ä¸­: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01<00:00, 20.12it/s]
```

---

## ãƒ‘ã‚¿ãƒ¼ãƒ³3: éåŒæœŸæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³

### æ¦‚è¦

I/Oå¾…æ©Ÿæ™‚é–“ã‚’æœ‰åŠ¹æ´»ç”¨ã—ã€è¤‡æ•°ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä¸¦è¡Œå‡¦ç†ã—ã¾ã™ã€‚

### é©ç”¨å ´é¢

- ä¸­ã€œé«˜ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ãªWebAPI
- éåŒæœŸãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆFastAPI, aiohttpï¼‰ã¨ã®çµ±åˆ
- ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸¦è¡Œå‘¼ã³å‡ºã—

### ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ

| ãƒ¡ãƒªãƒƒãƒˆ | ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ |
|---------|-----------|
| âœ… é«˜ã„ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ | âŒ å®Ÿè£…ãŒè¤‡é›‘ |
| âœ… ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡ãŒè‰¯ã„ | âŒ ãƒ‡ãƒãƒƒã‚°ãŒé›£ã—ã„ |
| âœ… ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ ãŒçŸ­ã„ | âŒ ä¸¦è¡Œåˆ¶å¾¡ãŒå¿…è¦ |

### å®Ÿè£…ä¾‹

#### asyncioãƒ™ãƒ¼ã‚¹ã®åŸºæœ¬å®Ÿè£…

```python
import asyncio
import onnxruntime as ort
import numpy as np
from typing import List
from concurrent.futures import ThreadPoolExecutor

class AsyncONNXPredictor:
    """éåŒæœŸæ¨è«–ã‚’è¡Œã†äºˆæ¸¬å™¨"""

    def __init__(self, model_path: str, max_workers: int = 4):
        """
        Args:
            model_path: ONNXãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
            max_workers: ä¸¦è¡Œå®Ÿè¡Œã™ã‚‹æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
        """
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        logger.info(f"AsyncPredictoråˆæœŸåŒ–: max_workers={max_workers}")

    async def predict_async(self, data: np.ndarray) -> np.ndarray:
        """
        éåŒæœŸã§æ¨è«–ã‚’å®Ÿè¡Œ

        Args:
            data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿

        Returns:
            äºˆæ¸¬çµæœ
        """
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            self.executor,
            self._predict_sync,
            data
        )
        return result

    def _predict_sync(self, data: np.ndarray) -> np.ndarray:
        """åŒæœŸçš„ãªæ¨è«–å‡¦ç†ï¼ˆExecutorã§å®Ÿè¡Œã•ã‚Œã‚‹ï¼‰"""
        if data.dtype != np.float32:
            data = data.astype(np.float32)

        outputs = self.session.run(None, {self.input_name: data})
        return outputs[0]

    async def predict_many_async(self, data_list: List[np.ndarray]) -> List[np.ndarray]:
        """
        è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚’éåŒæœŸä¸¦è¡Œå‡¦ç†

        Args:
            data_list: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ

        Returns:
            äºˆæ¸¬çµæœã®ãƒªã‚¹ãƒˆ
        """
        tasks = [self.predict_async(data) for data in data_list]
        results = await asyncio.gather(*tasks)
        return results

    def __del__(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        self.executor.shutdown(wait=True)

# ä½¿ç”¨ä¾‹
async def main():
    predictor = AsyncONNXPredictor("iris_model.onnx", max_workers=4)

    # å˜ä¸€ãƒ‡ãƒ¼ã‚¿ã®éåŒæœŸæ¨è«–
    sample = np.array([[5.1, 3.5, 1.4, 0.2]], dtype=np.float32)
    result = await predictor.predict_async(sample)
    print(f"äºˆæ¸¬çµæœ: {result[0]}")

    # è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã®ä¸¦è¡Œå‡¦ç†
    samples = [np.array([[5.1, 3.5, 1.4, 0.2]]) for _ in range(10)]
    results = await predictor.predict_many_async(samples)
    print(f"å‡¦ç†å®Œäº†: {len(results)}ä»¶")

if __name__ == "__main__":
    asyncio.run(main())
```

#### FastAPIçµ±åˆä¾‹

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List
import uvicorn

app = FastAPI(title="Iris ONNX Async API")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªäºˆæ¸¬å™¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
predictor: AsyncONNXPredictor = None

class PredictRequest(BaseModel):
    """äºˆæ¸¬ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    features: List[float]

    class Config:
        json_schema_extra = {
            "example": {
                "features": [5.1, 3.5, 1.4, 0.2]
            }
        }

class PredictResponse(BaseModel):
    """äºˆæ¸¬ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    prediction: int
    class_name: str

class BatchPredictRequest(BaseModel):
    """ãƒãƒƒãƒäºˆæ¸¬ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    samples: List[List[float]]

class BatchPredictResponse(BaseModel):
    """ãƒãƒƒãƒäºˆæ¸¬ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    predictions: List[int]
    count: int

@app.on_event("startup")
async def startup_event():
    """ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«äºˆæ¸¬å™¨ã‚’åˆæœŸåŒ–"""
    global predictor
    predictor = AsyncONNXPredictor("iris_model.onnx", max_workers=4)
    logger.info("äºˆæ¸¬å™¨ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")

@app.on_event("shutdown")
async def shutdown_event():
    """ã‚¢ãƒ—ãƒªçµ‚äº†æ™‚ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    global predictor
    if predictor:
        predictor.executor.shutdown(wait=True)
    logger.info("äºˆæ¸¬å™¨ã‚’ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã—ã¾ã—ãŸ")

@app.post("/predict", response_model=PredictResponse)
async def predict(request: PredictRequest):
    """
    å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«ã®äºˆæ¸¬

    - **features**: Irisã®4ã¤ã®ç‰¹å¾´é‡ [sepal_length, sepal_width, petal_length, petal_width]
    """
    try:
        # å…¥åŠ›æ¤œè¨¼
        if len(request.features) != 4:
            raise HTTPException(
                status_code=400,
                detail="ç‰¹å¾´é‡ã¯4ã¤å¿…è¦ã§ã™ï¼ˆsepal_length, sepal_width, petal_length, petal_widthï¼‰"
            )

        # æ¨è«–å®Ÿè¡Œ
        data = np.array([request.features], dtype=np.float32)
        result = await predictor.predict_async(data)

        # ã‚¯ãƒ©ã‚¹åãƒãƒƒãƒ”ãƒ³ã‚°
        class_names = ["setosa", "versicolor", "virginica"]
        prediction = int(result[0])

        return PredictResponse(
            prediction=prediction,
            class_name=class_names[prediction]
        )

    except Exception as e:
        logger.error(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/predict/batch", response_model=BatchPredictResponse)
async def predict_batch(request: BatchPredictRequest):
    """
    ãƒãƒƒãƒäºˆæ¸¬ï¼ˆéåŒæœŸä¸¦è¡Œå‡¦ç†ï¼‰

    - **samples**: äºˆæ¸¬ã—ãŸã„ã‚µãƒ³ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
    """
    try:
        # å…¥åŠ›æ¤œè¨¼
        for i, sample in enumerate(request.samples):
            if len(sample) != 4:
                raise HTTPException(
                    status_code=400,
                    detail=f"ã‚µãƒ³ãƒ—ãƒ«{i}ã®ç‰¹å¾´é‡æ•°ãŒä¸æ­£ã§ã™"
                )

        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        data_list = [
            np.array([sample], dtype=np.float32)
            for sample in request.samples
        ]

        # éåŒæœŸä¸¦è¡Œæ¨è«–
        results = await predictor.predict_many_async(data_list)
        predictions = [int(result[0]) for result in results]

        return BatchPredictResponse(
            predictions=predictions,
            count=len(predictions)
        )

    except Exception as e:
        logger.error(f"ãƒãƒƒãƒäºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {"status": "healthy", "model_loaded": predictor is not None}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

ä½¿ç”¨ä¾‹ï¼š

```bash
# ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
python async_api.py

# å˜ä¸€äºˆæ¸¬
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{"features": [5.1, 3.5, 1.4, 0.2]}'

# ãƒãƒƒãƒäºˆæ¸¬
curl -X POST "http://localhost:8000/predict/batch" \
  -H "Content-Type: application/json" \
  -d '{
    "samples": [
      [5.1, 3.5, 1.4, 0.2],
      [6.2, 2.9, 4.3, 1.3],
      [7.3, 2.9, 6.3, 1.8]
    ]
  }'
```

---

## ãƒ‘ã‚¿ãƒ¼ãƒ³4: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³

### æ¦‚è¦

ç¶™ç¶šçš„ã«æµã‚Œã¦ãã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å‡¦ç†ã—ã¾ã™ã€‚

### é©ç”¨å ´é¢

- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ˜ åƒåˆ†æ
- ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ç•°å¸¸æ¤œçŸ¥
- éŸ³å£°èªè­˜ãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç¿»è¨³
- IoTãƒ‡ãƒã‚¤ã‚¹ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒªãƒ¼ãƒ 
- ãƒ­ã‚°ç›£è¦–ãƒ»ç•°å¸¸æ¤œçŸ¥

### ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ

| ãƒ¡ãƒªãƒƒãƒˆ | ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ |
|---------|-----------|
| âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç† | âŒ å®Ÿè£…ãŒè¤‡é›‘ |
| âœ… ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· | âŒ çŠ¶æ…‹ç®¡ç†ãŒå¿…è¦ |
| âœ… ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ« | âŒ ã‚¨ãƒ©ãƒ¼ãƒªã‚«ãƒãƒªãŒé›£ã—ã„ |

### å®Ÿè£…ä¾‹

#### Queue + Threadingå®Ÿè£…

```python
import queue
import threading
import time
from typing import Callable, Optional

class StreamingPredictor:
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ¨è«–ã‚’è¡Œã†äºˆæ¸¬å™¨"""

    def __init__(
        self,
        model_path: str,
        callback: Callable[[np.ndarray, np.ndarray], None],
        max_queue_size: int = 100
    ):
        """
        Args:
            model_path: ONNXãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
            callback: æ¨è«–çµæœã‚’å‡¦ç†ã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•° (data, result) -> None
            max_queue_size: ã‚­ãƒ¥ãƒ¼ã®æœ€å¤§ã‚µã‚¤ã‚º
        """
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name
        self.callback = callback

        # ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¥ãƒ¼
        self.input_queue = queue.Queue(maxsize=max_queue_size)

        # ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰
        self.worker_thread = None
        self.is_running = False

        logger.info(f"StreamingPredictoråˆæœŸåŒ–: max_queue_size={max_queue_size}")

    def start(self):
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã‚’é–‹å§‹"""
        if self.is_running:
            logger.warning("æ—¢ã«å®Ÿè¡Œä¸­ã§ã™")
            return

        self.is_running = True
        self.worker_thread = threading.Thread(target=self._process_stream, daemon=True)
        self.worker_thread.start()
        logger.info("ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã—ãŸ")

    def stop(self):
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã‚’åœæ­¢"""
        if not self.is_running:
            return

        self.is_running = False
        if self.worker_thread:
            self.worker_thread.join(timeout=5)
        logger.info("ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã‚’åœæ­¢ã—ã¾ã—ãŸ")

    def push_data(self, data: np.ndarray, timeout: float = 1.0) -> bool:
        """
        æ¨è«–ã‚­ãƒ¥ãƒ¼ã«ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 

        Args:
            data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰

        Returns:
            è¿½åŠ ã«æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            self.input_queue.put(data, timeout=timeout)
            return True
        except queue.Full:
            logger.warning("ã‚­ãƒ¥ãƒ¼ãŒæº€æ¯ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return False

    def _process_stream(self):
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œã•ã‚Œã‚‹å‡¦ç†ãƒ«ãƒ¼ãƒ—"""
        logger.info("ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹")

        while self.is_running:
            try:
                # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
                data = self.input_queue.get(timeout=0.1)

                # æ¨è«–å®Ÿè¡Œ
                if data.dtype != np.float32:
                    data = data.astype(np.float32)

                outputs = self.session.run(None, {self.input_name: data})
                result = outputs[0]

                # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ
                try:
                    self.callback(data, result)
                except Exception as e:
                    logger.error(f"ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

                # ã‚­ãƒ¥ãƒ¼ã®ã‚¿ã‚¹ã‚¯å®Œäº†ã‚’é€šçŸ¥
                self.input_queue.task_done()

            except queue.Empty:
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ - ãƒ«ãƒ¼ãƒ—ã‚’ç¶šã‘ã‚‹
                continue
            except Exception as e:
                logger.error(f"ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

        logger.info("ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†")

# ä½¿ç”¨ä¾‹
def result_callback(data: np.ndarray, result: np.ndarray):
    """æ¨è«–çµæœã‚’å‡¦ç†ã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    print(f"å…¥åŠ›: {data[0]} -> äºˆæ¸¬: {result[0]}")

if __name__ == "__main__":
    # äºˆæ¸¬å™¨ã®åˆæœŸåŒ–
    predictor = StreamingPredictor(
        "iris_model.onnx",
        callback=result_callback,
        max_queue_size=100
    )

    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹
    predictor.start()

    try:
        # ãƒ‡ãƒ¼ã‚¿ã‚’ç¶™ç¶šçš„ã«é€ä¿¡ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        for i in range(20):
            sample = np.array([[5.1, 3.5, 1.4, 0.2]], dtype=np.float32)
            success = predictor.push_data(sample)
            if success:
                print(f"ãƒ‡ãƒ¼ã‚¿{i}ã‚’é€ä¿¡")
            time.sleep(0.5)  # 0.5ç§’ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿é€ä¿¡

        # å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ãŒå‡¦ç†ã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
        predictor.input_queue.join()

    finally:
        # åœæ­¢
        predictor.stop()
```

#### asyncio + aiokafkaçµ±åˆä¾‹

```python
import asyncio
from aiokafka import AIOKafkaConsumer, AIOKafkaProducer
import json
import numpy as np

class KafkaStreamingPredictor:
    """Kafkaã‚¹ãƒˆãƒªãƒ¼ãƒ ã¨çµ±åˆã—ãŸæ¨è«–å™¨"""

    def __init__(
        self,
        model_path: str,
        kafka_servers: List[str],
        input_topic: str,
        output_topic: str
    ):
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name

        self.kafka_servers = kafka_servers
        self.input_topic = input_topic
        self.output_topic = output_topic

        self.consumer = None
        self.producer = None

    async def start(self):
        """Kafkaã‚³ãƒ³ã‚·ãƒ¥ãƒ¼ãƒãƒ¼ãƒ»ãƒ—ãƒ­ãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼ã‚’èµ·å‹•"""
        # ã‚³ãƒ³ã‚·ãƒ¥ãƒ¼ãƒãƒ¼åˆæœŸåŒ–
        self.consumer = AIOKafkaConsumer(
            self.input_topic,
            bootstrap_servers=self.kafka_servers,
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
        await self.consumer.start()

        # ãƒ—ãƒ­ãƒ‡ãƒ¥ãƒ¼ã‚µãƒ¼åˆæœŸåŒ–
        self.producer = AIOKafkaProducer(
            bootstrap_servers=self.kafka_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        await self.producer.start()

        logger.info(f"Kafkaæ¥ç¶šå®Œäº†: {self.input_topic} -> {self.output_topic}")

    async def stop(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.consumer:
            await self.consumer.stop()
        if self.producer:
            await self.producer.stop()

    async def process_stream(self):
        """Kafkaã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å‡¦ç†"""
        try:
            async for message in self.consumer:
                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º
                data_dict = message.value
                features = data_dict.get("features")

                if not features or len(features) != 4:
                    logger.warning(f"ä¸æ­£ãªãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {data_dict}")
                    continue

                # æ¨è«–å®Ÿè¡Œ
                data = np.array([features], dtype=np.float32)
                outputs = self.session.run(None, {self.input_name: data})
                prediction = int(outputs[0][0])

                # çµæœã‚’Kafkaã«é€ä¿¡
                result = {
                    "input_features": features,
                    "prediction": prediction,
                    "timestamp": time.time()
                }
                await self.producer.send(self.output_topic, value=result)

                logger.debug(f"äºˆæ¸¬å®Œäº†: {features} -> {prediction}")

        except Exception as e:
            logger.error(f"ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise

# ä½¿ç”¨ä¾‹
async def main():
    predictor = KafkaStreamingPredictor(
        model_path="iris_model.onnx",
        kafka_servers=["localhost:9092"],
        input_topic="iris-input",
        output_topic="iris-predictions"
    )

    try:
        await predictor.start()
        await predictor.process_stream()
    finally:
        await predictor.stop()

if __name__ == "__main__":
    asyncio.run(main())
```

---

## ãƒ‘ã‚¿ãƒ¼ãƒ³5: REST APIãƒ‘ã‚¿ãƒ¼ãƒ³

### æ¦‚è¦

HTTPãƒ™ãƒ¼ã‚¹ã®RESTful APIã¨ã—ã¦æ¨è«–ã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚

### é©ç”¨å ´é¢

- Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã®çµ±åˆ
- ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- ç¤¾å†…APIã‚µãƒ¼ãƒ“ã‚¹
- ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ãƒ»MVPé–‹ç™º

### ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ

| ãƒ¡ãƒªãƒƒãƒˆ | ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ |
|---------|-----------|
| âœ… å®Ÿè£…ãŒç°¡å˜ | âŒ gRPCã‚ˆã‚Šä½é€Ÿ |
| âœ… ãƒ‡ãƒãƒƒã‚°ãŒå®¹æ˜“ | âŒ ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã®æ‰±ã„ãŒéåŠ¹ç‡ |
| âœ… åºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹ | âŒ ã‚¹ã‚­ãƒ¼ãƒå®šç¾©ãŒç·©ã„ |
| âœ… curlã§ç°¡å˜ã«ãƒ†ã‚¹ãƒˆå¯èƒ½ | |

### å®Ÿè£…ä¾‹

#### FastAPIå®Œå…¨å®Ÿè£…

```python
from fastapi import FastAPI, HTTPException, Depends, status
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, validator
from typing import List, Optional
import onnxruntime as ort
import numpy as np
import logging
import time

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPIã‚¢ãƒ—ãƒªåˆæœŸåŒ–
app = FastAPI(
    title="Iris Classification API",
    description="ONNX Runtime ã‚’ä½¿ç”¨ã—ãŸIrisåˆ†é¡API",
    version="1.0.0"
)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
_predictor: Optional['IrisPredictor'] = None

class IrisPredictor:
    """ONNXæ¨è«–ã‚¯ãƒ©ã‚¹"""

    CLASS_NAMES = ["setosa", "versicolor", "virginica"]

    def __init__(self, model_path: str):
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name
        logger.info(f"ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†: {model_path}")

    def predict(self, data: np.ndarray) -> dict:
        """æ¨è«–å®Ÿè¡Œ"""
        if data.dtype != np.float32:
            data = data.astype(np.float32)

        outputs = self.session.run(None, {self.input_name: data})
        prediction = int(outputs[0][0])

        # ç¢ºç‡æƒ…å ±ãŒã‚ã‚Œã°å«ã‚ã‚‹
        probabilities = outputs[1][0].tolist() if len(outputs) > 1 else None

        return {
            "prediction": prediction,
            "class_name": self.CLASS_NAMES[prediction],
            "probabilities": probabilities
        }

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«
class PredictRequest(BaseModel):
    """äºˆæ¸¬ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    sepal_length: float = Field(..., ge=0, le=10, description="ãŒãç‰‡ã®é•·ã• (cm)")
    sepal_width: float = Field(..., ge=0, le=10, description="ãŒãç‰‡ã®å¹… (cm)")
    petal_length: float = Field(..., ge=0, le=10, description="èŠ±ã³ã‚‰ã®é•·ã• (cm)")
    petal_width: float = Field(..., ge=0, le=10, description="èŠ±ã³ã‚‰ã®å¹… (cm)")

    class Config:
        json_schema_extra = {
            "example": {
                "sepal_length": 5.1,
                "sepal_width": 3.5,
                "petal_length": 1.4,
                "petal_width": 0.2
            }
        }

class PredictResponse(BaseModel):
    """äºˆæ¸¬ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    prediction: int = Field(..., description="äºˆæ¸¬ã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ID (0-2)")
    class_name: str = Field(..., description="äºˆæ¸¬ã•ã‚ŒãŸã‚¯ãƒ©ã‚¹å")
    probabilities: Optional[List[float]] = Field(None, description="å„ã‚¯ãƒ©ã‚¹ã®ç¢ºç‡")
    inference_time_ms: float = Field(..., description="æ¨è«–æ™‚é–“ (ãƒŸãƒªç§’)")

class BatchPredictRequest(BaseModel):
    """ãƒãƒƒãƒäºˆæ¸¬ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    samples: List[PredictRequest] = Field(..., min_length=1, max_length=100)

class BatchPredictResponse(BaseModel):
    """ãƒãƒƒãƒäºˆæ¸¬ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    results: List[PredictResponse]
    total_count: int
    total_inference_time_ms: float

class HealthResponse(BaseModel):
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    status: str
    model_loaded: bool
    timestamp: float

# ä¾å­˜æ€§æ³¨å…¥
def get_predictor() -> IrisPredictor:
    """äºˆæ¸¬å™¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    if _predictor is None:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="ãƒ¢ãƒ‡ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"
        )
    return _predictor

# ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚¤ãƒ™ãƒ³ãƒˆ
@app.on_event("startup")
async def startup_event():
    """èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
    global _predictor
    try:
        _predictor = IrisPredictor("models/iris_model.onnx")
        logger.info("APIã‚µãƒ¼ãƒãƒ¼èµ·å‹•å®Œäº†")
    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        raise

@app.on_event("shutdown")
async def shutdown_event():
    """çµ‚äº†æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    logger.info("APIã‚µãƒ¼ãƒãƒ¼ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³")

# ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/", tags=["General"])
async def root():
    """ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "message": "Iris Classification API",
        "version": "1.0.0",
        "docs": "/docs"
    }

@app.get("/health", response_model=HealthResponse, tags=["General"])
async def health_check():
    """
    ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯

    ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹ã¨APIã®ç¨¼åƒçŠ¶æ³ã‚’ç¢ºèªã—ã¾ã™ã€‚
    """
    return HealthResponse(
        status="healthy" if _predictor else "unhealthy",
        model_loaded=_predictor is not None,
        timestamp=time.time()
    )

@app.post(
    "/predict",
    response_model=PredictResponse,
    tags=["Prediction"],
    status_code=status.HTTP_200_OK
)
async def predict(
    request: PredictRequest,
    predictor: IrisPredictor = Depends(get_predictor)
):
    """
    å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«ã®äºˆæ¸¬

    Irisã®4ã¤ã®ç‰¹å¾´é‡ã‹ã‚‰å“ç¨®ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚

    - **sepal_length**: ãŒãç‰‡ã®é•·ã• (cm)
    - **sepal_width**: ãŒãç‰‡ã®å¹… (cm)
    - **petal_length**: èŠ±ã³ã‚‰ã®é•·ã• (cm)
    - **petal_width**: èŠ±ã³ã‚‰ã®å¹… (cm)
    """
    try:
        start_time = time.time()

        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        data = np.array([[
            request.sepal_length,
            request.sepal_width,
            request.petal_length,
            request.petal_width
        ]], dtype=np.float32)

        # æ¨è«–å®Ÿè¡Œ
        result = predictor.predict(data)

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“è¨ˆç®—
        inference_time_ms = (time.time() - start_time) * 1000

        return PredictResponse(
            prediction=result["prediction"],
            class_name=result["class_name"],
            probabilities=result["probabilities"],
            inference_time_ms=inference_time_ms
        )

    except Exception as e:
        logger.error(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"äºˆæ¸¬å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}"
        )

@app.post(
    "/predict/batch",
    response_model=BatchPredictResponse,
    tags=["Prediction"]
)
async def predict_batch(
    request: BatchPredictRequest,
    predictor: IrisPredictor = Depends(get_predictor)
):
    """
    ãƒãƒƒãƒäºˆæ¸¬

    è¤‡æ•°ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ã¾ã¨ã‚ã¦äºˆæ¸¬ã—ã¾ã™ï¼ˆæœ€å¤§100ä»¶ï¼‰ã€‚
    """
    try:
        start_time = time.time()
        results = []

        for sample in request.samples:
            sample_start = time.time()

            data = np.array([[
                sample.sepal_length,
                sample.sepal_width,
                sample.petal_length,
                sample.petal_width
            ]], dtype=np.float32)

            result = predictor.predict(data)
            sample_time_ms = (time.time() - sample_start) * 1000

            results.append(PredictResponse(
                prediction=result["prediction"],
                class_name=result["class_name"],
                probabilities=result["probabilities"],
                inference_time_ms=sample_time_ms
            ))

        total_time_ms = (time.time() - start_time) * 1000

        return BatchPredictResponse(
            results=results,
            total_count=len(results),
            total_inference_time_ms=total_time_ms
        )

    except Exception as e:
        logger.error(f"ãƒãƒƒãƒäºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ãƒãƒƒãƒäºˆæ¸¬å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}"
        )

# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
@app.exception_handler(ValueError)
async def value_error_handler(request, exc):
    return JSONResponse(
        status_code=status.HTTP_400_BAD_REQUEST,
        content={"detail": str(exc)}
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )
```

#### DockeråŒ–

```dockerfile
# Dockerfile
FROM python:3.13-slim

WORKDIR /app

# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ”ãƒ¼
COPY . .

# ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
RUN mkdir -p models

# ãƒãƒ¼ãƒˆå…¬é–‹
EXPOSE 8000

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:8000/health || exit 1

# èµ·å‹•ã‚³ãƒãƒ³ãƒ‰
CMD ["uvicorn", "api:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  iris-api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=info
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 3s
      retries: 3
```

#### ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µãƒ³ãƒ—ãƒ«

```python
import requests
import json

class IrisAPIClient:
    """Iris API ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url

    def predict(self, sepal_length: float, sepal_width: float,
                petal_length: float, petal_width: float) -> dict:
        """å˜ä¸€äºˆæ¸¬"""
        url = f"{self.base_url}/predict"
        data = {
            "sepal_length": sepal_length,
            "sepal_width": sepal_width,
            "petal_length": petal_length,
            "petal_width": petal_width
        }

        response = requests.post(url, json=data)
        response.raise_for_status()
        return response.json()

    def predict_batch(self, samples: list) -> dict:
        """ãƒãƒƒãƒäºˆæ¸¬"""
        url = f"{self.base_url}/predict/batch"
        data = {"samples": samples}

        response = requests.post(url, json=data)
        response.raise_for_status()
        return response.json()

    def health_check(self) -> dict:
        """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        url = f"{self.base_url}/health"
        response = requests.get(url)
        response.raise_for_status()
        return response.json()

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    client = IrisAPIClient()

    # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
    health = client.health_check()
    print(f"Health: {health}")

    # å˜ä¸€äºˆæ¸¬
    result = client.predict(5.1, 3.5, 1.4, 0.2)
    print(f"Prediction: {result['class_name']}")

    # ãƒãƒƒãƒäºˆæ¸¬
    samples = [
        {"sepal_length": 5.1, "sepal_width": 3.5, "petal_length": 1.4, "petal_width": 0.2},
        {"sepal_length": 6.2, "sepal_width": 2.9, "petal_length": 4.3, "petal_width": 1.3},
    ]
    batch_result = client.predict_batch(samples)
    print(f"Batch predictions: {batch_result['total_count']} samples")
```

---

## ãƒ‘ã‚¿ãƒ¼ãƒ³6: gRPCãƒ‘ã‚¿ãƒ¼ãƒ³

### æ¦‚è¦

Protocol Buffersã‚’ä½¿ã£ãŸé«˜æ€§èƒ½RPCãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚µãƒ¼ãƒ“ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚

### é©ç”¨å ´é¢

- ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹é–“é€šä¿¡
- é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒå¿…è¦ãªã‚·ã‚¹ãƒ†ãƒ 
- å¤šè¨€èªç’°å¢ƒï¼ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒè¤‡æ•°è¨€èªï¼‰
- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãŒå¿…è¦ãªå ´åˆ

### ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ

| ãƒ¡ãƒªãƒƒãƒˆ | ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ |
|---------|-----------|
| âœ… RESTã‚ˆã‚Šé«˜é€Ÿ | âŒ å­¦ç¿’ã‚³ã‚¹ãƒˆãŒé«˜ã„ |
| âœ… å‹å®‰å…¨ | âŒ ãƒ‡ãƒãƒƒã‚°ãŒé›£ã—ã„ |
| âœ… ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œ | âŒ ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰ç›´æ¥å‘¼ã¹ãªã„ |
| âœ… å¤šè¨€èªå¯¾å¿œ | âŒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒè¤‡é›‘ |

### å®Ÿè£…ä¾‹

#### Protocol Bufferså®šç¾©

```protobuf
// iris_service.proto
syntax = "proto3";

package iris;

// Irisåˆ†é¡ã‚µãƒ¼ãƒ“ã‚¹
service IrisClassifier {
  // å˜ä¸€äºˆæ¸¬
  rpc Predict(PredictRequest) returns (PredictResponse);

  // ãƒãƒƒãƒäºˆæ¸¬
  rpc PredictBatch(BatchPredictRequest) returns (BatchPredictResponse);

  // ã‚µãƒ¼ãƒãƒ¼ã‚µã‚¤ãƒ‰ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
  rpc PredictStream(stream PredictRequest) returns (stream PredictResponse);
}

// ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
message PredictRequest {
  float sepal_length = 1;
  float sepal_width = 2;
  float petal_length = 3;
  float petal_width = 4;
}

// ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
message PredictResponse {
  int32 prediction = 1;
  string class_name = 2;
  repeated float probabilities = 3;
  float inference_time_ms = 4;
}

// ãƒãƒƒãƒãƒªã‚¯ã‚¨ã‚¹ãƒˆ
message BatchPredictRequest {
  repeated PredictRequest samples = 1;
}

// ãƒãƒƒãƒãƒ¬ã‚¹ãƒãƒ³ã‚¹
message BatchPredictResponse {
  repeated PredictResponse results = 1;
  int32 total_count = 2;
  float total_inference_time_ms = 3;
}
```

#### gRPCã‚µãƒ¼ãƒãƒ¼å®Ÿè£…

```python
import grpc
from concurrent import futures
import time
import numpy as np
import onnxruntime as ort
import logging

# ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import iris_service_pb2
import iris_service_pb2_grpc

logger = logging.getLogger(__name__)

class IrisClassifierServicer(iris_service_pb2_grpc.IrisClassifierServicer):
    """Irisåˆ†é¡gRPCã‚µãƒ¼ãƒ“ã‚¹"""

    CLASS_NAMES = ["setosa", "versicolor", "virginica"]

    def __init__(self, model_path: str):
        """
        Args:
            model_path: ONNXãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        """
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name
        logger.info(f"gRPCã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–: {model_path}")

    def Predict(self, request, context):
        """å˜ä¸€äºˆæ¸¬"""
        try:
            start_time = time.time()

            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            data = np.array([[
                request.sepal_length,
                request.sepal_width,
                request.petal_length,
                request.petal_width
            ]], dtype=np.float32)

            # æ¨è«–
            outputs = self.session.run(None, {self.input_name: data})
            prediction = int(outputs[0][0])
            probabilities = outputs[1][0].tolist() if len(outputs) > 1 else []

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“
            inference_time_ms = (time.time() - start_time) * 1000

            return iris_service_pb2.PredictResponse(
                prediction=prediction,
                class_name=self.CLASS_NAMES[prediction],
                probabilities=probabilities,
                inference_time_ms=inference_time_ms
            )

        except Exception as e:
            logger.error(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return iris_service_pb2.PredictResponse()

    def PredictBatch(self, request, context):
        """ãƒãƒƒãƒäºˆæ¸¬"""
        try:
            start_time = time.time()
            results = []

            for sample in request.samples:
                sample_response = self.Predict(sample, context)
                results.append(sample_response)

            total_time_ms = (time.time() - start_time) * 1000

            return iris_service_pb2.BatchPredictResponse(
                results=results,
                total_count=len(results),
                total_inference_time_ms=total_time_ms
            )

        except Exception as e:
            logger.error(f"ãƒãƒƒãƒäºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return iris_service_pb2.BatchPredictResponse()

    def PredictStream(self, request_iterator, context):
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°äºˆæ¸¬"""
        try:
            for request in request_iterator:
                yield self.Predict(request, context)

        except Exception as e:
            logger.error(f"ã‚¹ãƒˆãƒªãƒ¼ãƒ äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))

def serve(model_path: str, port: int = 50051, max_workers: int = 10):
    """gRPCã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•"""
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=max_workers))

    iris_service_pb2_grpc.add_IrisClassifierServicer_to_server(
        IrisClassifierServicer(model_path), server
    )

    server.add_insecure_port(f'[::]:{port}')
    server.start()

    logger.info(f"gRPCã‚µãƒ¼ãƒãƒ¼èµ·å‹•: port={port}, max_workers={max_workers}")

    try:
        server.wait_for_termination()
    except KeyboardInterrupt:
        logger.info("ã‚µãƒ¼ãƒãƒ¼ã‚’åœæ­¢ã—ã¾ã™...")
        server.stop(grace=5)

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    serve("iris_model.onnx", port=50051)
```

#### gRPCã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå®Ÿè£…

```python
import grpc
import iris_service_pb2
import iris_service_pb2_grpc

class IrisGRPCClient:
    """Iris gRPCã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

    def __init__(self, host: str = "localhost", port: int = 50051):
        """
        Args:
            host: ã‚µãƒ¼ãƒãƒ¼ãƒ›ã‚¹ãƒˆ
            port: ã‚µãƒ¼ãƒãƒ¼ãƒãƒ¼ãƒˆ
        """
        self.channel = grpc.insecure_channel(f'{host}:{port}')
        self.stub = iris_service_pb2_grpc.IrisClassifierStub(self.channel)

    def predict(self, sepal_length: float, sepal_width: float,
                petal_length: float, petal_width: float) -> dict:
        """å˜ä¸€äºˆæ¸¬"""
        request = iris_service_pb2.PredictRequest(
            sepal_length=sepal_length,
            sepal_width=sepal_width,
            petal_length=petal_length,
            petal_width=petal_width
        )

        response = self.stub.Predict(request)

        return {
            "prediction": response.prediction,
            "class_name": response.class_name,
            "probabilities": list(response.probabilities),
            "inference_time_ms": response.inference_time_ms
        }

    def predict_batch(self, samples: list) -> dict:
        """ãƒãƒƒãƒäºˆæ¸¬"""
        requests = [
            iris_service_pb2.PredictRequest(**sample)
            for sample in samples
        ]

        batch_request = iris_service_pb2.BatchPredictRequest(samples=requests)
        response = self.stub.PredictBatch(batch_request)

        return {
            "results": [
                {
                    "prediction": r.prediction,
                    "class_name": r.class_name,
                    "probabilities": list(r.probabilities)
                }
                for r in response.results
            ],
            "total_count": response.total_count,
            "total_inference_time_ms": response.total_inference_time_ms
        }

    def predict_stream(self, samples: list):
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°äºˆæ¸¬"""
        def request_generator():
            for sample in samples:
                yield iris_service_pb2.PredictRequest(**sample)

        responses = self.stub.PredictStream(request_generator())

        for response in responses:
            yield {
                "prediction": response.prediction,
                "class_name": response.class_name,
                "probabilities": list(response.probabilities),
                "inference_time_ms": response.inference_time_ms
            }

    def close(self):
        """æ¥ç¶šã‚’ã‚¯ãƒ­ãƒ¼ã‚º"""
        self.channel.close()

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    client = IrisGRPCClient()

    # å˜ä¸€äºˆæ¸¬
    result = client.predict(5.1, 3.5, 1.4, 0.2)
    print(f"äºˆæ¸¬: {result['class_name']}")

    # ãƒãƒƒãƒäºˆæ¸¬
    samples = [
        {"sepal_length": 5.1, "sepal_width": 3.5, "petal_length": 1.4, "petal_width": 0.2},
        {"sepal_length": 6.2, "sepal_width": 2.9, "petal_length": 4.3, "petal_width": 1.3},
    ]
    batch_result = client.predict_batch(samples)
    print(f"ãƒãƒƒãƒäºˆæ¸¬å®Œäº†: {batch_result['total_count']}ä»¶")

    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
    print("ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°äºˆæ¸¬:")
    for result in client.predict_stream(samples):
        print(f"  - {result['class_name']}")

    client.close()
```

---

## ãƒ‘ã‚¿ãƒ¼ãƒ³7: ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³

### æ¦‚è¦

AWS Lambdaã€Azure Functionsã€Google Cloud Functionsãªã©ã®ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ç’°å¢ƒã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¾ã™ã€‚

### é©ç”¨å ´é¢

- ã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•å‹ã®æ¨è«–
- ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ãŒä¸å®šæœŸãƒ»æ€¥å¢—ã™ã‚‹å ´åˆ
- ã‚¤ãƒ³ãƒ•ãƒ©ç®¡ç†ã‚’æœ€å°åŒ–ã—ãŸã„å ´åˆ
- ã‚³ã‚¹ãƒˆæœ€é©åŒ–ï¼ˆä½¿ã£ãŸåˆ†ã ã‘èª²é‡‘ï¼‰

### ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ

| ãƒ¡ãƒªãƒƒãƒˆ | ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ |
|---------|-----------|
| âœ… è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° | âŒ ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆé…å»¶ |
| âœ… ã‚¤ãƒ³ãƒ•ãƒ©ç®¡ç†ä¸è¦ | âŒ å®Ÿè¡Œæ™‚é–“åˆ¶é™ï¼ˆ15åˆ†ç­‰ï¼‰ |
| âœ… å¾“é‡èª²é‡‘ | âŒ ãƒ¡ãƒ¢ãƒªåˆ¶é™ |
| âœ… é«˜å¯ç”¨æ€§ | âŒ ãƒ‡ãƒãƒƒã‚°ãŒé›£ã—ã„ |

### å®Ÿè£…ä¾‹

#### AWS Lambdaå®Ÿè£…

```python
# lambda_handler.py
import json
import boto3
import numpy as np
import onnxruntime as ort
import os
import logging

logger = logging.getLogger()
logger.setLevel(logging.INFO)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼ˆã‚³ãƒ³ãƒ†ãƒŠå†åˆ©ç”¨æ™‚ã«ä¿æŒã•ã‚Œã‚‹ï¼‰
_session = None

def load_model():
    """S3ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’åˆæœŸåŒ–"""
    global _session

    if _session is not None:
        logger.info("æ—¢å­˜ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å†åˆ©ç”¨")
        return _session

    logger.info("ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")

    # S3ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    s3 = boto3.client('s3')
    bucket = os.environ['MODEL_BUCKET']
    key = os.environ['MODEL_KEY']
    local_path = '/tmp/model.onnx'

    s3.download_file(bucket, key, local_path)
    logger.info(f"ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {local_path}")

    # ONNX Runtimeã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–
    _session = ort.InferenceSession(local_path)
    logger.info("ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–å®Œäº†")

    return _session

def lambda_handler(event, context):
    """
    Lambdaé–¢æ•°ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ

    Args:
        event: API Gatewayã‹ã‚‰ã®ã‚¤ãƒ™ãƒ³ãƒˆ
        context: Lambdaå®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        API Gatewayãƒ¬ã‚¹ãƒãƒ³ã‚¹
    """
    try:
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆåˆå›ã¾ãŸã¯ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆæ™‚ï¼‰
        session = load_model()
        input_name = session.get_inputs()[0].name

        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã‚’ãƒ‘ãƒ¼ã‚¹
        body = json.loads(event['body'])

        # å…¥åŠ›æ¤œè¨¼
        features = body.get('features')
        if not features or len(features) != 4:
            return {
                'statusCode': 400,
                'body': json.dumps({
                    'error': 'ç‰¹å¾´é‡ã¯4ã¤å¿…è¦ã§ã™'
                })
            }

        # æ¨è«–å®Ÿè¡Œ
        data = np.array([features], dtype=np.float32)
        outputs = session.run(None, {input_name: data})
        prediction = int(outputs[0][0])
        probabilities = outputs[1][0].tolist() if len(outputs) > 1 else None

        # ã‚¯ãƒ©ã‚¹åãƒãƒƒãƒ”ãƒ³ã‚°
        class_names = ["setosa", "versicolor", "virginica"]

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        return {
            'statusCode': 200,
            'headers': {
                'Content-Type': 'application/json',
                'Access-Control-Allow-Origin': '*'
            },
            'body': json.dumps({
                'prediction': prediction,
                'class_name': class_names[prediction],
                'probabilities': probabilities
            })
        }

    except Exception as e:
        logger.error(f"ã‚¨ãƒ©ãƒ¼: {str(e)}", exc_info=True)
        return {
            'statusCode': 500,
            'body': json.dumps({
                'error': str(e)
            })
        }
```

#### Dockerfileãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆLambdaç”¨ï¼‰

```dockerfile
# Dockerfile
FROM public.ecr.aws/lambda/python:3.13

# ONNX Runtimeã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
RUN pip install --no-cache-dir \
    onnxruntime==1.23.0 \
    boto3 \
    numpy

# Lambdaé–¢æ•°ã‚³ãƒ¼ãƒ‰ã‚’ã‚³ãƒ”ãƒ¼
COPY lambda_handler.py ${LAMBDA_TASK_ROOT}

# ãƒãƒ³ãƒ‰ãƒ©ãƒ¼æŒ‡å®š
CMD ["lambda_handler.lambda_handler"]
```

#### SAMãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆInfrastructure as Codeï¼‰

```yaml
# template.yaml
AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: Iris ONNX Inference Lambda

Globals:
  Function:
    Timeout: 30
    MemorySize: 1024

Resources:
  # Lambdaé–¢æ•°
  IrisPredictionFunction:
    Type: AWS::Serverless::Function
    Properties:
      PackageType: Image
      ImageUri: !Sub '${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/iris-lambda:latest'
      Environment:
        Variables:
          MODEL_BUCKET: !Ref ModelBucket
          MODEL_KEY: iris_model.onnx
      Events:
        PredictAPI:
          Type: Api
          Properties:
            Path: /predict
            Method: post
      Policies:
        - S3ReadPolicy:
            BucketName: !Ref ModelBucket

  # S3ãƒã‚±ãƒƒãƒˆï¼ˆãƒ¢ãƒ‡ãƒ«ä¿å­˜ç”¨ï¼‰
  ModelBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: iris-model-bucket
      VersioningConfiguration:
        Status: Enabled

Outputs:
  PredictionApi:
    Description: "API Gateway endpoint URL"
    Value: !Sub "https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/predict/"

  FunctionArn:
    Description: "Lambda Function ARN"
    Value: !GetAtt IrisPredictionFunction.Arn
```

#### ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †

```bash
# 1. Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ãƒ“ãƒ«ãƒ‰
docker build -t iris-lambda .

# 2. ECRã«ãƒ—ãƒƒã‚·ãƒ¥
aws ecr create-repository --repository-name iris-lambda
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin <account-id>.dkr.ecr.us-east-1.amazonaws.com
docker tag iris-lambda:latest <account-id>.dkr.ecr.us-east-1.amazonaws.com/iris-lambda:latest
docker push <account-id>.dkr.ecr.us-east-1.amazonaws.com/iris-lambda:latest

# 3. ãƒ¢ãƒ‡ãƒ«ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
aws s3 cp iris_model.onnx s3://iris-model-bucket/iris_model.onnx

# 4. SAMã§ãƒ‡ãƒ—ãƒ­ã‚¤
sam build
sam deploy --guided

# 5. ãƒ†ã‚¹ãƒˆ
curl -X POST https://<api-id>.execute-api.us-east-1.amazonaws.com/Prod/predict \
  -H "Content-Type: application/json" \
  -d '{"features": [5.1, 3.5, 1.4, 0.2]}'
```

---

## ãƒ‘ã‚¿ãƒ¼ãƒ³é¸æŠã‚¬ã‚¤ãƒ‰

### ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ

```
æ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é¸ã¶
  |
  â”œâ”€ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§ãŒæœ€é‡è¦ï¼Ÿ
  |    â”œâ”€ Yes â†’ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ (4)
  |    â””â”€ No â†’ æ¬¡ã¸
  |
  â”œâ”€ å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬å‡¦ç†ï¼Ÿ
  |    â”œâ”€ Yes â†’ ãƒãƒƒãƒæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ (2)
  |    â””â”€ No â†’ æ¬¡ã¸
  |
  â”œâ”€ Webã‹ã‚‰å‘¼ã³å‡ºã™ï¼Ÿ
  |    â”œâ”€ Yes â†’ REST APIãƒ‘ã‚¿ãƒ¼ãƒ³ (5)
  |    â””â”€ No â†’ æ¬¡ã¸
  |
  â”œâ”€ é«˜æ€§èƒ½ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ï¼Ÿ
  |    â”œâ”€ Yes â†’ gRPCãƒ‘ã‚¿ãƒ¼ãƒ³ (6)
  |    â””â”€ No â†’ æ¬¡ã¸
  |
  â”œâ”€ ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ãŒä¸å®šæœŸï¼Ÿ
  |    â”œâ”€ Yes â†’ ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ (7)
  |    â””â”€ No â†’ æ¬¡ã¸
  |
  â”œâ”€ ä¸¦è¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ï¼Ÿ
  |    â”œâ”€ Yes â†’ éåŒæœŸæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ (3)
  |    â””â”€ No â†’ åŒæœŸæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ (1)
```

### ã‚·ãƒŠãƒªã‚ªåˆ¥æ¨å¥¨ãƒ‘ã‚¿ãƒ¼ãƒ³

| ã‚·ãƒŠãƒªã‚ª | æ¨å¥¨ãƒ‘ã‚¿ãƒ¼ãƒ³ | ç†ç”± |
|---------|------------|------|
| **ç¤¾å†…ãƒ„ãƒ¼ãƒ«ï¼ˆCLIï¼‰** | 1. åŒæœŸæ¨è«– | ã‚·ãƒ³ãƒ—ãƒ«ã€ãƒ‡ãƒãƒƒã‚°å®¹æ˜“ |
| **Webã‚¢ãƒ—ãƒª** | 5. REST API | å®Ÿè£…ç°¡å˜ã€åºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹ |
| **ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³** | 2. ãƒãƒƒãƒæ¨è«– | é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã€åŠ¹ç‡çš„ |
| **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç•°å¸¸æ¤œçŸ¥** | 4. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚° | ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã€ç¶™ç¶šå‡¦ç† |
| **é«˜ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯API** | 3. éåŒæœŸæ¨è«– | ä¸¦è¡Œå‡¦ç†ã€åŠ¹ç‡çš„ |
| **ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹** | 6. gRPC | é«˜æ€§èƒ½ã€å‹å®‰å…¨ |
| **ä¸å®šæœŸãƒãƒƒãƒå‡¦ç†** | 7. ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ | è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒ«ã€ã‚³ã‚¹ãƒˆæœ€é© |

---

## å®Ÿè£…é †åºã®æ¨å¥¨

å­¦ç¿’ãƒ»å®Ÿè£…ã®é †åºã¨ã—ã¦ä»¥ä¸‹ã‚’æ¨å¥¨ã—ã¾ã™ï¼š

### ã‚¹ãƒ†ãƒƒãƒ—1: åŸºç¤ã‚’å›ºã‚ã‚‹
1. **åŒæœŸæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³** (1) â† **iris_sklearn_svcã§å®Ÿè£…æ¸ˆã¿**
   - ONNXã®åŸºæœ¬ã‚’ç†è§£
   - æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã§ç†è§£ã—ã‚„ã™ã„
   - æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®åœŸå°

### ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š
2. **ãƒãƒƒãƒæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³** (2)
   - åŒæœŸæ¨è«–ã‚’æ‹¡å¼µ
   - ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šã®åŸºæœ¬ã‚’å­¦ã¶

### ã‚¹ãƒ†ãƒƒãƒ—3: APIåŒ–
3. **REST APIãƒ‘ã‚¿ãƒ¼ãƒ³** (5)
   - å®Ÿç”¨çš„ãªã‚µãƒ¼ãƒ“ã‚¹åŒ–
   - FastAPIã®å­¦ç¿’
   - ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ»é‹ç”¨ã®åŸºç¤

### ã‚¹ãƒ†ãƒƒãƒ—4: ä¸¦è¡Œå‡¦ç†
4. **éåŒæœŸæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³** (3)
   - asyncioã®ç†è§£
   - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
   - REST APIã«çµ„ã¿è¾¼ã‚ã‚‹

### ã‚¹ãƒ†ãƒƒãƒ—5: é«˜åº¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
5. **ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ¨è«–** (4) - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ãŒå¿…è¦ãªå ´åˆ
6. **gRPCãƒ‘ã‚¿ãƒ¼ãƒ³** (6) - ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ãŒå¿…è¦ãªå ´åˆ
7. **ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³** (7) - ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ‡ãƒ—ãƒ­ã‚¤ãŒå¿…è¦ãªå ´åˆ

---

## ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã®æœ€é©åŒ–

#### âŒ æ‚ªã„ä¾‹ï¼šæ¯å›ãƒ­ãƒ¼ãƒ‰
```python
def predict(data):
    session = ort.InferenceSession('model.onnx')  # æ¯å›ãƒ­ãƒ¼ãƒ‰ï¼ˆé…ã„ï¼‰
    return session.run(None, {input_name: data})
```

#### âœ… è‰¯ã„ä¾‹ï¼šåˆæœŸåŒ–æ™‚ã«1åº¦ã ã‘ãƒ­ãƒ¼ãƒ‰
```python
class Predictor:
    def __init__(self, model_path):
        self.session = ort.InferenceSession(model_path)  # 1åº¦ã ã‘
        self.input_name = self.session.get_inputs()[0].name

    def predict(self, data):
        return self.session.run(None, {self.input_name: data})
```

### 2. å…¥åŠ›æ¤œè¨¼

```python
def validate_input(data: np.ndarray, expected_shape: tuple) -> None:
    """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œè¨¼"""
    # æ¬¡å…ƒãƒã‚§ãƒƒã‚¯
    if data.ndim != len(expected_shape):
        raise ValueError(f"æœŸå¾…ã•ã‚Œã‚‹æ¬¡å…ƒ: {len(expected_shape)}, å®Ÿéš›: {data.ndim}")

    # å½¢çŠ¶ãƒã‚§ãƒƒã‚¯ï¼ˆå‹•çš„æ¬¡å…ƒã‚’é™¤ãï¼‰
    for i, (expected, actual) in enumerate(zip(expected_shape, data.shape)):
        if expected != -1 and expected != actual:
            raise ValueError(f"æ¬¡å…ƒ{i}: æœŸå¾…={expected}, å®Ÿéš›={actual}")

    # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
    if np.isnan(data).any():
        raise ValueError("å…¥åŠ›ã«NaNãŒå«ã¾ã‚Œã¦ã„ã¾ã™")

    # ç„¡é™å¤§ãƒã‚§ãƒƒã‚¯
    if np.isinf(data).any():
        raise ValueError("å…¥åŠ›ã«infãŒå«ã¾ã‚Œã¦ã„ã¾ã™")
```

### 3. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
import functools
import logging

def handle_prediction_errors(func):
    """äºˆæ¸¬ã‚¨ãƒ©ãƒ¼ã‚’ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã™ã‚‹ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except ValueError as e:
            logging.error(f"å…¥åŠ›ã‚¨ãƒ©ãƒ¼: {e}")
            raise
        except ort.capi.onnxruntime_pybind11_state.RuntimeException as e:
            logging.error(f"ONNXãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
            raise
        except Exception as e:
            logging.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            raise
    return wrapper

class Predictor:
    @handle_prediction_errors
    def predict(self, data):
        # æ¨è«–å‡¦ç†
        pass
```

### 4. ãƒ­ã‚°ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

```python
import time
import logging

class InstrumentedPredictor:
    """ãƒ­ã‚°ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²ã™ã‚‹äºˆæ¸¬å™¨"""

    def __init__(self, model_path):
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name
        self.logger = logging.getLogger(__name__)

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        self.total_predictions = 0
        self.total_time = 0.0

    def predict(self, data):
        start_time = time.time()

        try:
            result = self.session.run(None, {self.input_name: data})

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            elapsed = time.time() - start_time
            self.total_predictions += 1
            self.total_time += elapsed

            # ãƒ­ã‚°
            self.logger.info(
                f"äºˆæ¸¬å®Œäº† | "
                f"æ™‚é–“: {elapsed*1000:.2f}ms | "
                f"å…¥åŠ›å½¢çŠ¶: {data.shape}"
            )

            return result

        except Exception as e:
            self.logger.error(f"äºˆæ¸¬å¤±æ•—: {e}")
            raise

    def get_metrics(self):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—"""
        if self.total_predictions == 0:
            return {"avg_time_ms": 0, "total_predictions": 0}

        return {
            "avg_time_ms": (self.total_time / self.total_predictions) * 1000,
            "total_predictions": self.total_predictions
        }
```

### 5. ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†

```python
import json
from pathlib import Path

class VersionedPredictor:
    """ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’ç®¡ç†ã™ã‚‹äºˆæ¸¬å™¨"""

    def __init__(self, model_path: str, metadata_path: str = None):
        self.model_path = model_path
        self.session = ort.InferenceSession(model_path)
        self.input_name = self.session.get_inputs()[0].name

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        if metadata_path:
            with open(metadata_path) as f:
                self.metadata = json.load(f)
        else:
            self.metadata = {}

    def get_version_info(self):
        """ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’è¿”ã™"""
        return {
            "model_path": self.model_path,
            "model_version": self.metadata.get("version", "unknown"),
            "model_date": self.metadata.get("created_at", "unknown"),
            "framework": self.metadata.get("framework", "unknown"),
            "accuracy": self.metadata.get("accuracy", "unknown")
        }

    def predict(self, data):
        return self.session.run(None, {self.input_name: data})
```

ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ä¾‹ï¼ˆ`model_metadata.json`ï¼‰:
```json
{
  "version": "1.0.0",
  "created_at": "2025-01-15T10:30:00Z",
  "framework": "scikit-learn 1.6.0",
  "accuracy": 0.97,
  "dataset": "iris",
  "features": ["sepal_length", "sepal_width", "petal_length", "petal_width"],
  "classes": ["setosa", "versicolor", "virginica"]
}
```

---

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### å•é¡Œ1: "No such file or directory" ã‚¨ãƒ©ãƒ¼

**ç—‡çŠ¶**:
```
FileNotFoundError: [Errno 2] No such file or directory: 'model.onnx'
```

**åŸå› **: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ãŒé–“é•ã£ã¦ã„ã‚‹

**è§£æ±ºç­–**:
```python
from pathlib import Path

# çµ¶å¯¾ãƒ‘ã‚¹ã‚’ä½¿ç”¨
model_path = Path(__file__).parent / "models" / "iris_model.onnx"
if not model_path.exists():
    raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")

session = ort.InferenceSession(str(model_path))
```

### å•é¡Œ2: å…¥åŠ›å½¢çŠ¶ã®ãƒŸã‚¹ãƒãƒƒãƒ

**ç—‡çŠ¶**:
```
RuntimeException: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT :
Invalid rank for input: X. Got: 1 Expected: 2
```

**åŸå› **: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¬¡å…ƒãŒæœŸå¾…ã¨ç•°ãªã‚‹

**è§£æ±ºç­–**:
```python
# 1æ¬¡å…ƒé…åˆ—ã®å ´åˆã€2æ¬¡å…ƒã«å¤‰æ›
if data.ndim == 1:
    data = data.reshape(1, -1)

# ã¾ãŸã¯
data = np.array([[5.1, 3.5, 1.4, 0.2]])  # æœ€åˆã‹ã‚‰2æ¬¡å…ƒã«ã™ã‚‹
```

### å•é¡Œ3: å‹ã®ä¸ä¸€è‡´

**ç—‡çŠ¶**:
```
RuntimeException: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT :
Unexpected input data type. Actual: (tensor(double)) , expected: (tensor(float))
```

**åŸå› **: ãƒ‡ãƒ¼ã‚¿å‹ãŒ float64 ã ãŒã€ãƒ¢ãƒ‡ãƒ«ã¯ float32 ã‚’æœŸå¾…

**è§£æ±ºç­–**:
```python
# float32ã«å¤‰æ›
data = data.astype(np.float32)

# ã¾ãŸã¯ä½œæˆæ™‚ã«å‹ã‚’æŒ‡å®š
data = np.array([[5.1, 3.5, 1.4, 0.2]], dtype=np.float32)
```

### å•é¡Œ4: ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼

**ç—‡çŠ¶**:
```
MemoryError: Unable to allocate array
```

**åŸå› **: ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå¤§ãã™ãã‚‹

**è§£æ±ºç­–**:
```python
# ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹
BATCH_SIZE = 32  # 64ã‹ã‚‰32ã«å‰Šæ¸›

# ã¾ãŸã¯ãƒ¡ãƒ¢ãƒªç›£è¦–ã‚’è¿½åŠ 
import psutil

def get_memory_usage():
    """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰"""
    process = psutil.Process()
    return process.memory_info().rss / 1024 / 1024

# ãƒãƒƒãƒå‡¦ç†å‰ã«ãƒã‚§ãƒƒã‚¯
mem_before = get_memory_usage()
result = predictor.predict_batch(large_batch)
mem_after = get_memory_usage()
print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {mem_after - mem_before:.2f}MB")
```

### å•é¡Œ5: æ¨è«–ãŒé…ã„

**åŸå› ã¨è§£æ±ºç­–**:

| åŸå›  | è§£æ±ºç­– |
|------|--------|
| æ¯å›ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ | åˆæœŸåŒ–æ™‚ã«1åº¦ã ã‘ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¾ãŸã¯ã‚¯ãƒ©ã‚¹å¤‰æ•°ï¼‰ |
| CPUå®Ÿè¡Œ | GPUç‰ˆONNX Runtimeã‚’ä½¿ç”¨ |
| å°ã•ã„ãƒãƒƒãƒã‚µã‚¤ã‚º | ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤§ããã™ã‚‹ |
| ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ | äº‹å‰ã«é©åˆ‡ãªå‹ã«å¤‰æ› |

```python
# GPUä½¿ç”¨
import onnxruntime as ort

providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
session = ort.InferenceSession('model.onnx', providers=providers)

# ä½¿ç”¨ä¸­ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç¢ºèª
print(session.get_providers())
```

---

## ã¾ã¨ã‚

### é‡è¦ãƒã‚¤ãƒ³ãƒˆ

1. **ãƒ‘ã‚¿ãƒ¼ãƒ³é¸æŠã¯è¦ä»¶æ¬¡ç¬¬**
   - ã¾ãšã¯åŒæœŸæ¨è«–ã§åŸºç¤ã‚’å›ºã‚ã‚‹
   - è¦ä»¶ã«å¿œã˜ã¦é©åˆ‡ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é¸ã¶

2. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®éµ**
   - ãƒ¢ãƒ‡ãƒ«ã¯1åº¦ã ã‘ãƒ­ãƒ¼ãƒ‰
   - é©åˆ‡ãªãƒãƒƒãƒã‚µã‚¤ã‚º
   - å‹å¤‰æ›ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’æœ€å°åŒ–

3. **æœ¬ç•ªç’°å¢ƒã§ã®æ³¨æ„ç‚¹**
   - å…¥åŠ›æ¤œè¨¼ã¯å¿…é ˆ
   - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’ä¸å¯§ã«
   - ãƒ­ã‚°ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚’å¿˜ã‚Œãšã«

4. **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**
   - ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã«å¿œã˜ã¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é¸ã¶
   - éåŒæœŸå‡¦ç†ã‚„ãƒãƒƒãƒå‡¦ç†ã‚’æ´»ç”¨
   - å¿…è¦ã«å¿œã˜ã¦ã‚µãƒ¼ãƒãƒ¼ãƒ¬ã‚¹åŒ–

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

- [ ] iris_sklearn_svcã®åŒæœŸæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å¾©ç¿’
- [ ] ãƒãƒƒãƒæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®Ÿè£…
- [ ] FastAPIã§REST APIåŒ–
- [ ] éåŒæœŸæ¨è«–ã‚’è©¦ã™
- [ ] èˆˆå‘³ã«å¿œã˜ã¦é«˜åº¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã«æŒ‘æˆ¦

---

**æœ€çµ‚æ›´æ–°**: 2025-01-15
**é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: `github_actions_guide.md`, `project_overview.md`
