#!/usr/bin/env python3
"""
Irisåˆ†é¡ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

TensorFlow/Kerasã‚’ä½¿ç”¨ã—ã¦Irisåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã€
TensorFlow Servingç”¨ã®SavedModelå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

Usage:
    python build_model.py
"""

import argparse
import os
from typing import Tuple

import numpy as np
import tensorflow as tf
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


def load_iris_data() -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Irisãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚“ã§è¨“ç·´ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²ã™ã‚‹

    Returns:
        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
            (X_train, X_test, y_train, y_test)
    """
    print("Loading Iris dataset...")
    iris = datasets.load_iris()
    X = iris.data.astype(np.float32)
    y = iris.target

    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²ï¼ˆ80:20ï¼‰
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"Training samples: {len(X_train)}")
    print(f"Test samples: {len(X_test)}")

    return X_train, X_test, y_train, y_test


def normalize_data(
    X_train: np.ndarray, X_test: np.ndarray
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–ã™ã‚‹ï¼ˆå¹³å‡0ã€åˆ†æ•£1ï¼‰

    Args:
        X_train: è¨“ç·´ãƒ‡ãƒ¼ã‚¿
        X_test: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿

    Returns:
        Tuple[np.ndarray, np.ndarray]: æ­£è¦åŒ–ã•ã‚ŒãŸ (X_train, X_test)
    """
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    return X_train_scaled.astype(np.float32), X_test_scaled.astype(np.float32)


def build_model(input_shape: int, num_classes: int) -> tf.keras.Model:
    """
    TensorFlow/Kerasãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹

    Args:
        input_shape: å…¥åŠ›ç‰¹å¾´é‡ã®æ¬¡å…ƒæ•°
        num_classes: ã‚¯ãƒ©ã‚¹æ•°

    Returns:
        tf.keras.Model: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
    """
    print("Building model...")

    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(input_shape,), name="input"),
        tf.keras.layers.Dense(64, activation="relu", name="dense1"),
        tf.keras.layers.Dropout(0.2, name="dropout1"),
        tf.keras.layers.Dense(32, activation="relu", name="dense2"),
        tf.keras.layers.Dropout(0.2, name="dropout2"),
        tf.keras.layers.Dense(num_classes, activation="softmax", name="output"),
    ])

    model.compile(
        optimizer="adam",
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"],
    )

    model.summary()

    return model


def train_model(
    model: tf.keras.Model,
    X_train: np.ndarray,
    y_train: np.ndarray,
    X_test: np.ndarray,
    y_test: np.ndarray,
    epochs: int = 100,
    batch_size: int = 16,
) -> tf.keras.callbacks.History:
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹

    Args:
        model: å­¦ç¿’å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«
        X_train: è¨“ç·´ãƒ‡ãƒ¼ã‚¿
        y_train: è¨“ç·´ãƒ©ãƒ™ãƒ«
        X_test: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        y_test: ãƒ†ã‚¹ãƒˆãƒ©ãƒ™ãƒ«
        epochs: ã‚¨ãƒãƒƒã‚¯æ•°
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º

    Returns:
        tf.keras.callbacks.History: å­¦ç¿’å±¥æ­´
    """
    print("Training model...")

    # Early Stoppingã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    early_stopping = tf.keras.callbacks.EarlyStopping(
        monitor="val_loss",
        patience=10,
        restore_best_weights=True,
    )

    history = model.fit(
        X_train,
        y_train,
        validation_data=(X_test, y_test),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stopping],
        verbose=1,
    )

    return history


def evaluate_model(
    model: tf.keras.Model, X_test: np.ndarray, y_test: np.ndarray
) -> None:
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã™ã‚‹

    Args:
        model: è©•ä¾¡å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«
        X_test: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        y_test: ãƒ†ã‚¹ãƒˆãƒ©ãƒ™ãƒ«
    """
    print("\nEvaluating model...")
    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
    print(f"Test Loss: {test_loss:.4f}")
    print(f"Test Accuracy: {test_accuracy:.4f}")


def export_model(model: tf.keras.Model, export_path: str) -> None:
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’SavedModelå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹

    TensorFlow Servingç”¨ã®serving signatureã‚’å®šç¾©ã—ã¾ã™ã€‚

    Args:
        model: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«
        export_path: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
    """
    print(f"\nExporting model to {export_path}...")

    # SavedModelå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    # TensorFlow Servingã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ "serving_default" signatureã‚’ä½¿ç”¨
    tf.saved_model.save(model, export_path)

    print(f"Model exported successfully to {export_path}")

    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’è¡¨ç¤º
    print("\nSavedModel structure:")
    os.system(f"ls -lh {export_path}")

    # Signatureæƒ…å ±ã‚’è¡¨ç¤º
    print("\nModel signatures:")
    os.system(f"saved_model_cli show --dir {export_path} --tag_set serve --signature_def serving_default")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(description="Build and export Iris classification model")
    parser.add_argument(
        "--export-dir",
        type=str,
        default="saved_model/iris",
        help="Directory to export SavedModel (default: saved_model/iris)",
    )
    parser.add_argument(
        "--version",
        type=int,
        default=1,
        help="Model version number (default: 1)",
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=100,
        help="Number of training epochs (default: 100)",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=16,
        help="Batch size for training (default: 16)",
    )

    args = parser.parse_args()

    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ‘ã‚¹ã‚’æ§‹ç¯‰ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã‚’å«ã‚€ï¼‰
    export_path = os.path.join(args.export_dir, str(args.version))

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    X_train, X_test, y_train, y_test = load_iris_data()

    # ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–
    X_train, X_test = normalize_data(X_train, X_test)

    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    model = build_model(input_shape=4, num_classes=3)

    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    history = train_model(
        model,
        X_train,
        y_train,
        X_test,
        y_test,
        epochs=args.epochs,
        batch_size=args.batch_size,
    )

    # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
    evaluate_model(model, X_test, y_test)

    # ãƒ¢ãƒ‡ãƒ«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    export_model(model, export_path)

    print("\nâœ… Model creation completed successfully!")
    print(f"ğŸ“¦ SavedModel location: {export_path}")
    print("\nğŸ“ Next steps:")
    print("  1. Build Docker image with TensorFlow Serving")
    print("  2. Run TensorFlow Serving container")
    print("  3. Test inference with gRPC and REST clients")


if __name__ == "__main__":
    main()
